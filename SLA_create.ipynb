{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca612f-2130-4823-8724-8e67d0ec001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartsheet # need version 2.105.0\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import pyodbc\n",
    "import smartsheet\n",
    "import re\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, Float, inspect\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "api_key = 'smartsheet_key'\n",
    "sheet_id = 'smartsheet_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894eea18-9fc6-4878-b7b3-964229208014",
   "metadata": {},
   "source": [
    "# Load the required Jira tables to get the records for SLAs that have not completed processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ff4fe-7764-4113-bbec-4f60c7d7ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection settings\n",
    "server = 'server_path'\n",
    "database = 'database_name'\n",
    "username = 'username'\n",
    "password = 'password'\n",
    "view_name_bic_issues = 'dbo.tablename'\n",
    "view_name_bic_top_level_issues = 'dbo.tablename'\n",
    "\n",
    "# Establishing the database connection\n",
    "conn_str = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# SQL query to fetch the view data\n",
    "query1 = f'SELECT * FROM {view_name_bic_issues}'\n",
    "query2 = f'SELECT * FROM {view_name_bic_top_level_issues}'\n",
    "\n",
    "# Executing the query and fetching the data\n",
    "bic_issues = pd.read_sql_query(query1, conn)\n",
    "top_level_issues = pd.read_sql_query(query2, conn)\n",
    "\n",
    "# Closing the database connection\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4efe4-a35f-4559-bc1e-0b540948055f",
   "metadata": {},
   "source": [
    "# Target records with SLA in the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9768277-2b85-4aea-853f-af8ad6663817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter project keys to RDS\n",
    "rds = bic_issues[bic_issues.Project_Key == 'RDS']\n",
    "\n",
    "def check_for_sla(text):\n",
    "    if 'SLA' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# add a column that identifies if the 'Issue_Summary' field contains the text SLA and returns a 1 if SLA is in the text\n",
    "rds['contains_sla'] = rds.apply(lambda x: check_for_sla(x.Issue_Summary), axis=1)\n",
    "\n",
    "# filter to only SLA records \n",
    "sla_records = rds[rds.contains_sla == 1]\n",
    "\n",
    "# filter sla records based on the date which the records should only be newly requested\n",
    "# as prior records have been left open\n",
    "date_from = pd.to_datetime('05/18/2023')\n",
    "sla_records['Created_Date'] = pd.to_datetime(sla_records['Created_Date'])\n",
    "sla_records = sla_records[sla_records['Created_Date'] >= date_from] #### pulled for testing \n",
    "#Issue_Ticket_Number = [2191, 2606, 2611]\n",
    "#sla_records = sla_records[sla_records.Issue_Ticket_Number.isin(Issue_Ticket_Number)]\n",
    "len(sla_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d661ee-a12d-4e64-9786-dc646e5967c3",
   "metadata": {},
   "source": [
    "# Merge required fields from the parent ticket and update field titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153287fe-16ac-4566-b5af-cc6bbbee5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in required fields from the parent ticket\n",
    "top_slice = top_level_issues[['Issue_ID',\n",
    "                              'Issue_Status',\n",
    "                              'Issue_Ticket_Number',\n",
    "                              'Project_Key',\n",
    "                              'Research_Info__Project_Title',\n",
    "                              'Research_Info__PI_Email',\n",
    "                              'Research_Info__PI_Name']]\n",
    "top_slice.rename(columns={'Issue_ID':'Parent_Issue_ID',\n",
    "                          'Issue_Ticket_Number':'Issue_Ticket_Number_parent', \n",
    "                          'Issue_Status':'Issue_Status_parent',\n",
    "                          'Project_Key':'Project_Key_parent',\n",
    "                          'Research_Info__Project_Title':'Research_Info__Project_Title_parent',\n",
    "                          'Research_Info__PI_Email':'Research_Info__PI_Email_parent',\n",
    "                          'Research_Info__PI_Name':'Research_Info__PI_Name_parent'}, inplace=True)\n",
    "\n",
    "\n",
    "top_slice['Project_Key_parent_full'] = top_slice.apply(lambda x: x.Project_Key_parent +\"-\"+ str(x.Issue_Ticket_Number_parent), axis=1) \n",
    "\n",
    "sla_merged = pd.merge(sla_records, top_slice, how='left', on='Parent_Issue_ID')\n",
    "\n",
    "sla_merged['Project_Key_child_full'] = sla_merged.apply(lambda x: x.Project_Key +\"-\"+ str(x.Issue_Ticket_Number), axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a028ee-a991-42c0-bdd0-de7b0cc65f7e",
   "metadata": {},
   "source": [
    "# Parse the text data in the SLA Jira ticket to extract the required SLA features to build the SLA and apply \"Jira Data Missing\" via QC methods if there are missing values in the Jira ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0bb5a-c12c-4377-9ec3-9f26cc012b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_support_data(text):\n",
    "    '''\n",
    "    scope: parses the required data from the SLA subtask Issue_Description that has been formated to collect the scope and other\n",
    "            data required for the SLA downstream processing\n",
    "    '''\n",
    "    # Initialize the dictionary with default values\n",
    "    support_data = {\n",
    "        \"scope\": \"Jira Data Missing\",\n",
    "        \"start\": \"Jira Data Missing\",\n",
    "        \"end\": \"Jira Data Missing\",\n",
    "        \"hours\": 0,\n",
    "        \"r1\": 0,\n",
    "        \"r2\": 0\n",
    "    }\n",
    "    \n",
    "    if text is not None:\n",
    "        # Use regular expressions to extract data\n",
    "        scope_match = re.search(r'^(.*?)###', text, re.DOTALL | re.IGNORECASE)\n",
    "        if scope_match:\n",
    "            support_data[\"scope\"] = scope_match.group(1).strip()\n",
    "\n",
    "        start_match = re.search(r'Start:\\s*(\\d{2}/\\d{2}/\\d{4})', text)\n",
    "        if start_match:\n",
    "            support_data[\"start\"] = start_match.group(1)\n",
    "\n",
    "        end_match = re.search(r'End:\\s*(\\d{2}/\\d{2}/\\d{4})', text)\n",
    "        if end_match:\n",
    "            support_data[\"end\"] = end_match.group(1)\n",
    "\n",
    "        hours_match = re.search(r'Total Hours:\\s*(\\d+)', text)\n",
    "        if hours_match:\n",
    "            support_data[\"hours\"] = int(hours_match.group(1))\n",
    "\n",
    "        r1_match = re.search(r'Additional Resource 1:\\s*(.*?)\\s*(Additional Resource|$)', text, re.DOTALL)\n",
    "        if r1_match:\n",
    "            r1_text = r1_match.group(1).strip()\n",
    "            support_data[\"r1\"] = r1_text if r1_text else 0\n",
    "\n",
    "        r2_match = re.search(r'Additional Resource 2:\\s*(.*?)\\s*(Additional Resource|$)', text, re.DOTALL)\n",
    "        if r2_match:\n",
    "            r2_text = r2_match.group(1).strip()\n",
    "            support_data[\"r2\"] = r2_text if r2_text else 0\n",
    "\n",
    "    return support_data\n",
    "\n",
    "\n",
    "def sla_state_qc(row):\n",
    "    '''\n",
    "    scope: quality control check to determine if a manaul review is needed for required fields that\n",
    "            are being derived from the Jira data and ETL to Smartsheets\n",
    "    \n",
    "    '''\n",
    "    if row['estimated_project_start_date'] == 'Jira Data Missing':\n",
    "        results = 'Jira Data Missing'\n",
    "        \n",
    "    elif row['estimated_project_end_date'] == 'Jira Data Missing':\n",
    "        results = 'Jira Data Missing'\n",
    "        \n",
    "    elif row['total_estimated_hours'] == 0:\n",
    "        results = 'Jira Data Missing'\n",
    "        \n",
    "    elif row['scope'] == '':\n",
    "        results = 'Jira Data Missing'\n",
    "        \n",
    "    else:\n",
    "        results = 'Format Review Required'\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# add the project scope data and other supporting data needed for downstream SLA processing\n",
    "sla_merged['scope'] = sla_merged.apply(lambda x: parse_support_data(x.Issue_Description)['scope'],axis=1)\n",
    "sla_merged['estimated_project_start_date'] = sla_merged.apply(lambda x: parse_support_data(x.Issue_Description)['start'],axis=1)\n",
    "sla_merged['estimated_project_end_date'] = sla_merged.apply(lambda x: parse_support_data(x.Issue_Description)['end'],axis=1)\n",
    "sla_merged['total_estimated_hours'] = sla_merged.apply(lambda x: parse_support_data(x.Issue_Description)['hours'],axis=1)\n",
    "sla_merged['additional_resource_1'] = sla_merged.apply(lambda x: parse_support_data(x.Issue_Description)['r1'],axis=1)\n",
    "sla_merged['additional_resource_2'] = sla_merged.apply(lambda x: parse_support_data(x.Issue_Description)['r2'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Replace \"None\" with empty string (\"\") and remove records that have \"Jira Data Missing\" from the scope\n",
    "sla_merged = sla_merged.fillna('Jira Data Missing')\n",
    "sla_merged = sla_merged[sla_merged.scope != 'Jira Data Missing']\n",
    "sla_merged['SLA_state'] = sla_merged.apply(sla_state_qc, axis=1)   \n",
    "len(sla_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e64ba-1333-495b-a2d1-b4066c460e95",
   "metadata": {},
   "source": [
    "# Scope down the record set and update field titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1896d1c0-4662-4e63-bf6c-32bc5add929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope down the dataframe to only the required fields then preprocess so the field titles match what is in smartsheets \n",
    "scoped_records = sla_merged[['SLA_state',\n",
    "                             'Project_Key_parent_full',\n",
    "                             'Project_Key_child_full',\n",
    "                             'Research_Info__Project_Title_parent',\n",
    "                             'scope',\n",
    "                             'estimated_project_start_date',\n",
    "                             'estimated_project_end_date',\n",
    "                             'Research_Info__PI_Name_parent',\n",
    "                             'Research_Info__PI_Email_parent',\n",
    "                             'total_estimated_hours',\n",
    "                             'additional_resource_1',\n",
    "                             'additional_resource_2']]\n",
    "\n",
    "\n",
    "scoped_records.rename(columns={'Project_Key_parent_full':'jira_ticket_number',\n",
    "                               'Project_Key_child_full':'jira_subtask_ticket_number',\n",
    "                               'Research_Info__Project_Title_parent':'project_title',\n",
    "                               'Research_Info__PI_Name_parent':'pi_name',\n",
    "                               'Research_Info__PI_Email_parent':'pi_email'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda8405-d402-4529-9801-4aa071020c1a",
   "metadata": {},
   "source": [
    "# Create additional SLA required calculated fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc9381-eb91-4dba-aeaa-b952aac3e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert specific columns to float data type\n",
    "columns_to_convert = ['total_estimated_hours', 'additional_resource_1', 'additional_resource_2']\n",
    "scoped_records[columns_to_convert] = scoped_records[columns_to_convert].astype(float)\n",
    "\n",
    "# add in the other fields within the smartsheet and derive values\n",
    "def total_hourly_cost(hours, gratis, rate):\n",
    "    #\n",
    "    result = (hours-gratis)*rate\n",
    "    if result < 0:\n",
    "        output = 0\n",
    "    else:\n",
    "        output = result\n",
    "    return output \n",
    "\n",
    "def total_estimated_cost(hour_cost, r1, r2):\n",
    "    \n",
    "    result = hour_cost+ r1 + r2\n",
    "    if result < 0:\n",
    "        output = 0\n",
    "    else:\n",
    "        output = result\n",
    "    return output \n",
    "\n",
    "\n",
    "scoped_records['gratis_hours'] = 8\n",
    "scoped_records['hourly_rate'] = 60\n",
    "scoped_records['total_hourly_cost'] = scoped_records.apply(lambda x: total_hourly_cost(x.total_estimated_hours, x.gratis_hours, x.hourly_rate ), axis=1) \n",
    "scoped_records['total_estimated_cost'] = scoped_records.apply(lambda x: total_estimated_cost(x.total_hourly_cost,x.additional_resource_1,x.additional_resource_2), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1481ede-411b-4b24-987b-6af0adfbac7e",
   "metadata": {},
   "source": [
    "# Load the current smartsheet record and use the \"jira_ticket_number\" field to create a unique list of the tickets that have already been posted and remove these from the scoped record set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabfd275-c8cd-4dcc-b597-cf013e16a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_and_load_to_dataframe(api_token, sheet_id):\n",
    "    # Initialize the Smartsheet client\n",
    "    ss_client = smartsheet.Smartsheet(api_token)\n",
    "\n",
    "    try:\n",
    "        # Get the sheet information to access column details\n",
    "        sheet = ss_client.Sheets.get_sheet(sheet_id)\n",
    "        columns = sheet.columns\n",
    "\n",
    "        # Extract column headers from the sheet\n",
    "        column_headers = [column.title for column in columns]\n",
    "\n",
    "        # Load the entire sheet\n",
    "        sheet = ss_client.Sheets.get_sheet(sheet_id, page_size=10000)\n",
    "        rows = sheet.rows\n",
    "\n",
    "        # Create a list to hold row data\n",
    "        data = []\n",
    "\n",
    "        # Extract data from each row and add to the list\n",
    "        for row in rows:\n",
    "            row_data = []\n",
    "            for cell in row.cells:\n",
    "                row_data.append(cell.display_value)\n",
    "            data.append(row_data)\n",
    "\n",
    "        # Convert the list to a DataFrame with column headers\n",
    "        df = pd.DataFrame(data, columns=column_headers)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "current_smartsheet_df = extract_and_load_to_dataframe(api_key, sheet_id)\n",
    "jira_ticket_list = list(set(list(current_smartsheet_df.jira_ticket_number)))\n",
    "\n",
    "scoped_records = scoped_records[~scoped_records.jira_ticket_number.isin(jira_ticket_list)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37faf3ae-0987-4c36-abeb-7d5050383424",
   "metadata": {},
   "source": [
    "# Get the current listing of field names and unique IDs to match the field name from the scoped data to smartsheet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc0f0e-dd81-462a-90a9-900c3048785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_name_to_id(sheet_id, api_key):\n",
    "    # Initialize the Smartsheet client\n",
    "    smartsheet_client = smartsheet.Smartsheet(api_key)\n",
    "\n",
    "    try:\n",
    "        # Load the specified sheet\n",
    "        sheet = smartsheet_client.Sheets.get_sheet(sheet_id)\n",
    "\n",
    "        # Initialize an empty dictionary to store column name to column ID mapping\n",
    "        column_name_to_id = {}\n",
    "\n",
    "        # Iterate through columns in the sheet\n",
    "        for column in sheet.columns:\n",
    "            # Store the column name (title) as the key and column ID as the value\n",
    "            column_name_to_id[column.title] = column.id\n",
    "\n",
    "        return column_name_to_id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Call the function to get the column name to column ID mapping\n",
    "result = get_column_name_to_id(sheet_id, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2fbea-d660-48e5-b376-bd3035f1f6ae",
   "metadata": {},
   "source": [
    "# If scope records exist then load the data into Smartsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f203cc6-4d89-4136-9feb-98b665ef6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(scoped_records) >0:\n",
    "    # Initialize the Smartsheet client\n",
    "    smartsheet_client = smartsheet.Smartsheet(api_key)\n",
    "\n",
    "    # Convert all columns to string data type\n",
    "    new_records = scoped_records.astype(str)\n",
    "\n",
    "    new_records_columns = list(set(list(scoped_records)))\n",
    "    df_len = len(new_records)\n",
    "    counter = 0\n",
    "    # use a counter to iterate over the rows within the dataframe to load the data from each row into the smartsheet format\n",
    "    while counter < df_len:\n",
    "        # Specify cell values for one row\n",
    "        row_a = smartsheet.models.Row()\n",
    "        row_a.to_top = True\n",
    "        # iterate over the field names in the scoped dataset and use these field names to call the \n",
    "        # key from the result dictionary that contains the smartsheet field ID associated with the key/title\n",
    "        # and use this field ID to build the record data neede for smartsheet to post the row\n",
    "        for column_name in new_records_columns:\n",
    "            row_a.cells.append({\n",
    "                'column_id': int(result[column_name]), \n",
    "                'value': new_records.iloc[counter][column_name]\n",
    "            })\n",
    "        # Add rows to sheet\n",
    "        response = smartsheet_client.Sheets.add_rows(\n",
    "            sheet_id=sheet_id,  \n",
    "            list_of_rows=[row_a],\n",
    "        )\n",
    "\n",
    "        # Check for success or handle errors as needed\n",
    "        if response.message == 'SUCCESS':\n",
    "            print('Rows added successfully')\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            print(f'Error: {response.message}')\n",
    "            counter = counter + 1\n",
    "else:\n",
    "    print('No Records to Process')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
